{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a43609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 2s 29ms/step - loss: 1.1246 - accuracy: 0.5513 - val_loss: 0.8721 - val_accuracy: 0.6371 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.9641 - accuracy: 0.6185 - val_loss: 0.8488 - val_accuracy: 0.6680 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.9350 - accuracy: 0.6251 - val_loss: 0.8258 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.8523 - accuracy: 0.6637 - val_loss: 0.8039 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.8117 - accuracy: 0.6560 - val_loss: 0.7819 - val_accuracy: 0.7259 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7678 - accuracy: 0.6770 - val_loss: 0.7612 - val_accuracy: 0.7336 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.7019 - accuracy: 0.7211 - val_loss: 0.7417 - val_accuracy: 0.7490 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.7233 - val_loss: 0.7207 - val_accuracy: 0.7529 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.7321 - val_loss: 0.6992 - val_accuracy: 0.7645 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5873 - accuracy: 0.7729 - val_loss: 0.6777 - val_accuracy: 0.7799 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5788 - accuracy: 0.7971 - val_loss: 0.6554 - val_accuracy: 0.8031 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5622 - accuracy: 0.8004 - val_loss: 0.6320 - val_accuracy: 0.8340 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.8236 - val_loss: 0.6079 - val_accuracy: 0.8456 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.8445 - val_loss: 0.5830 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8467 - val_loss: 0.5568 - val_accuracy: 0.8842 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4451 - accuracy: 0.8556 - val_loss: 0.5295 - val_accuracy: 0.8996 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8754 - val_loss: 0.5018 - val_accuracy: 0.9189 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8820 - val_loss: 0.4746 - val_accuracy: 0.9344 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3612 - accuracy: 0.9085 - val_loss: 0.4460 - val_accuracy: 0.9537 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.9173 - val_loss: 0.4167 - val_accuracy: 0.9575 - lr: 0.0010\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Adjusted LSTM Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94        60\n",
      "         1.0       0.98      0.90      0.94        71\n",
      "\n",
      "    accuracy                           0.94       131\n",
      "   macro avg       0.94      0.94      0.94       131\n",
      "weighted avg       0.94      0.94      0.94       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load datasets\n",
    "errors_df = pd.read_csv('PdM_errors.csv')\n",
    "failures_df = pd.read_csv('PdM_failures.csv')\n",
    "machines_df = pd.read_csv('PdM_machines.csv')\n",
    "maint_df = pd.read_csv('PdM_maint.csv')\n",
    "telemetry_df = pd.read_csv('PdM_telemetry.csv')\n",
    "\n",
    "# Convert datetime columns to datetime type\n",
    "errors_df['datetime'] = pd.to_datetime(errors_df['datetime'])\n",
    "failures_df['datetime'] = pd.to_datetime(failures_df['datetime'])\n",
    "maint_df['datetime'] = pd.to_datetime(maint_df['datetime'])\n",
    "telemetry_df['datetime'] = pd.to_datetime(telemetry_df['datetime'])\n",
    "\n",
    "# Add target variable 'failure' to indicate any failure in next month\n",
    "failures_df['failure'] = 1\n",
    "\n",
    "# Merge all relevant data on machineID and datetime\n",
    "df = telemetry_df.copy()\n",
    "df = df.merge(machines_df, on='machineID', how='left')\n",
    "df = df.merge(errors_df, on=['datetime', 'machineID'], how='left')\n",
    "df = df.merge(maint_df, on=['datetime', 'machineID'], how='left')\n",
    "df = df.merge(failures_df[['datetime', 'machineID', 'failure']], on=['datetime', 'machineID'], how='left')\n",
    "df['failure'] = df['failure'].fillna(0)  # Set failures to 0 where there's no failure\n",
    "\n",
    "# Fill missing categorical values and encode categorical columns\n",
    "df['errorID'] = df['errorID'].fillna('no_error')\n",
    "df['comp'] = df['comp'].fillna('no_maintenance')\n",
    "label_encoder = LabelEncoder()\n",
    "df['errorID'] = label_encoder.fit_transform(df['errorID'])\n",
    "df['comp'] = label_encoder.fit_transform(df['comp'])\n",
    "df['model'] = label_encoder.fit_transform(df['model'])\n",
    "\n",
    "# Advanced Feature Engineering: Adding Rolling Statistics and Lag Features\n",
    "for window in [3, 6, 12]:  # Advanced windows in months\n",
    "    for feature in ['volt', 'rotate', 'pressure', 'vibration']:\n",
    "        df[f'{feature}_rolling_mean_{window}'] = df.groupby('machineID')[feature].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        df[f'{feature}_rolling_std_{window}'] = df.groupby('machineID')[feature].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "\n",
    "# Track failure frequency and maintenance trends over last 6 months\n",
    "df['failure_last_6m'] = df.groupby('machineID')['failure'].transform(lambda x: x.rolling(6, min_periods=1).sum())\n",
    "df['maint_freq_last_6m'] = df.groupby('machineID')['comp'].transform(lambda x: x.rolling(6, min_periods=1).count())\n",
    "\n",
    "# Monthly Aggregation\n",
    "df['month'] = df['datetime'].dt.to_period('M')\n",
    "agg_funcs = {\n",
    "    'volt': ['mean', 'std'],\n",
    "    'rotate': ['mean', 'std'],\n",
    "    'pressure': ['mean', 'std'],\n",
    "    'vibration': ['mean', 'std'],\n",
    "    'age': 'max',\n",
    "    'errorID': 'nunique',\n",
    "    'comp': 'nunique',\n",
    "    'failure_last_6m': 'max',\n",
    "    'maint_freq_last_6m': 'max'\n",
    "}\n",
    "features = df.groupby(['machineID', 'month']).agg(agg_funcs)\n",
    "features.columns = ['_'.join(col) for col in features.columns]\n",
    "features = features.reset_index()\n",
    "\n",
    "# Target variable creation\n",
    "df['next_month'] = df['month'] + 1\n",
    "failures_next_month = df[df['failure'] == 1][['machineID', 'next_month']].drop_duplicates()\n",
    "failures_next_month['failure_in_next_month'] = 1\n",
    "features = features.merge(failures_next_month, left_on=['machineID', 'month'], right_on=['machineID', 'next_month'], how='left')\n",
    "features['failure_in_next_month'] = features['failure_in_next_month'].fillna(0)\n",
    "features.drop('next_month', axis=1, inplace=True)\n",
    "\n",
    "# Data Scaling\n",
    "scaler = StandardScaler()\n",
    "X = features.drop(['failure_in_next_month', 'machineID', 'month'], axis=1)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = features['failure_in_next_month'].values\n",
    "\n",
    "# Convert data to sequential format\n",
    "def create_sequences(X, y, time_steps=3):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y)\n",
    "\n",
    "# Train-test-validation split for time series\n",
    "train_size = int(len(X_seq) * 0.7)\n",
    "val_size = int(len(X_seq) * 0.2)\n",
    "X_train = X_seq[:train_size]\n",
    "y_train = y_seq[:train_size]\n",
    "X_val = X_seq[train_size:train_size + val_size]\n",
    "y_val = y_seq[train_size:train_size + val_size]\n",
    "X_test = X_seq[train_size + val_size:]\n",
    "y_test = y_seq[train_size + val_size:]\n",
    "\n",
    "# Adjusted LSTM Model with Higher Regularization and Dropout\n",
    "model = Sequential()\n",
    "model.add(LSTM(12, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False,\n",
    "               kernel_regularizer=l2(0.01)))  # Reduce LSTM units and increase L2 regularization\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Higher dropout rate for stronger regularization\n",
    "\n",
    "# Adding a dense layer with L2 regularization\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Adjust as needed based on your model's performance\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    class_weight=class_weights_dict,  # Ensure class weights are applied\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"Adjusted LSTM Model Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f80c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 750us/step\n",
      "Classification Report for Test Data:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94        60\n",
      "         1.0       0.98      0.90      0.94        71\n",
      "\n",
      "    accuracy                           0.94       131\n",
      "   macro avg       0.94      0.94      0.94       131\n",
      "weighted avg       0.94      0.94      0.94       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's predictions on the test data\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Print the classification report for the test data\n",
    "print(\"Classification Report for Test Data:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfce42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
